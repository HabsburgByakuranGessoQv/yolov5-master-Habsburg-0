# YOLOv5 ğŸš€ by Ultralytics, AGPL-3.0 license
"""
Run YOLOv5 detection inference on images, videos, directories, globs, YouTube, webcam, streams, etc.

Usage - sources:
    $ python detect.py --weights yolov5s.pt --source 0                               # webcam
                                                     img.jpg                         # image
                                                     vid.mp4                         # video
                                                     screen                          # screenshot
                                                     path/                           # directory
                                                     list.txt                        # list of images
                                                     list.streams                    # list of streams
                                                     'path/*.jpg'                    # glob
                                                     'https://youtu.be/Zgi9g1ksQHc'  # YouTube
                                                     'rtsp://example.com/media.mp4'  # RTSP, RTMP, HTTP stream

Usage - formats:
    $ python detect.py --weights yolov5s.pt                 # PyTorch
                                 yolov5s.torchscript        # TorchScript
                                 yolov5s.onnx               # ONNX Runtime or OpenCV DNN with --dnn
                                 yolov5s_openvino_model     # OpenVINO
                                 yolov5s.engine             # TensorRT
                                 yolov5s.mlmodel            # CoreML (macOS-only)
                                 yolov5s_saved_model        # TensorFlow SavedModel
                                 yolov5s.pb                 # TensorFlow GraphDef
                                 yolov5s.tflite             # TensorFlow Lite
                                 yolov5s_edgetpu.tflite     # TensorFlow Edge TPU
                                 yolov5s_paddle_model       # PaddlePaddle
"""

import argparse
import os
import platform
import sys
from pathlib import Path

import torch

FILE = Path(__file__).resolve()
ROOT = FILE.parents[0]  # YOLOv5 root directory
if str(ROOT) not in sys.path:
    sys.path.append(str(ROOT))  # add ROOT to PATH
ROOT = Path(os.path.relpath(ROOT, Path.cwd()))  # relative

from models.common import DetectMultiBackend
from utils.dataloaders import IMG_FORMATS, VID_FORMATS, LoadImages, LoadScreenshots, LoadStreams
from utils.general import (LOGGER, Profile, check_file, check_img_size, check_imshow, check_requirements, colorstr, cv2,
                           increment_path, non_max_suppression, print_args, scale_boxes, strip_optimizer, xyxy2xywh)
from utils.plots import Annotator, colors, save_one_box
from utils.torch_utils import select_device, smart_inference_mode


@smart_inference_mode()
def run(
        weights=ROOT / 'yolov5s.pt',  # model path or triton URL
        source=ROOT / 'data/images',  # file/dir/URL/glob/screen/0(webcam)
        data=ROOT / 'data/coco128.yaml',  # datasets.yaml path
        imgsz=(640, 640),  # inference size (height, width)
        conf_thres=0.25,  # confidence threshold
        iou_thres=0.45,  # NMS IOU threshold
        max_det=1000,  # maximum detections per image
        device='',  # cuda device, i.e. 0 or 0,1,2,3 or cpu
        view_img=False,  # show results
        save_txt=False,  # save results to *.txt
        save_conf=False,  # save confidences in --save-txt labels
        save_crop=False,  # save cropped prediction boxes
        nosave=False,  # do not save images/videos
        classes=None,  # filter by class: --class 0, or --class 0 2 3
        agnostic_nms=False,  # class-agnostic NMS
        augment=False,  # augmented inference
        visualize=False,  # visualize features
        update=False,  # update all models
        project=ROOT / 'runs/detect',  # save results to project/name
        name='exp',  # save results to project/name
        exist_ok=False,  # existing project/name ok, do not increment
        line_thickness=3,  # bounding box thickness (pixels)
        hide_labels=False,  # hide labels
        hide_conf=False,  # hide confidences
        half=False,  # use FP16 half-precision inference
        dnn=False,  # use OpenCV DNN for ONNX inference
        vid_stride=1,  # video frame-rate stride
):
    source = str(source)
    # å¦‚æœnosaveä¸ºFalseï¼ˆæ„å‘³ç€å…è®¸ä¿å­˜å›¾åƒï¼‰ä¸”sourceä¸ä»¥.txtç»“å°¾ï¼Œé‚£ä¹ˆå°†è®¾ç½®save_imgä¸ºTrue
    save_img = not nosave and not source.endswith('.txt')  # save inference images
    # æ£€æŸ¥sourceæ˜¯å¦ä¸ºä¸€ä¸ªæ–‡ä»¶ã€‚å®ƒè·å–sourceçš„åç¼€å¹¶æ£€æŸ¥å®ƒæ˜¯å¦åœ¨å›¾ç‰‡æ ¼å¼å’Œè§†é¢‘æ ¼å¼åˆ—è¡¨ä¸­
    is_file = Path(source).suffix[1:] in (IMG_FORMATS + VID_FORMATS)
    is_url = source.lower().startswith(('rtsp://', 'rtmp://', 'http://', 'https://'))
    webcam = source.isnumeric() or source.endswith('.streams') or (is_url and not is_file)
    screenshot = source.lower().startswith('screen')
    if is_url and is_file:
        # å¦‚æœsourceæ˜¯URLä¸”åŒæ—¶æ˜¯ä¸€ä¸ªæ–‡ä»¶ï¼Œé‚£ä¹ˆå°†è°ƒç”¨check_fileå‡½æ•°æ¥ä¸‹è½½æ–‡ä»¶
        source = check_file(source)  # download

    # Directories
    # é€šè¿‡increment_pathå‡½æ•°åˆ›å»ºä¸€ä¸ªæ–°çš„ä¿å­˜ç›®å½•è·¯å¾„ã€‚Path(project)è¡¨ç¤ºé¡¹ç›®çš„è·¯å¾„ï¼Œnameæ˜¯æ–‡ä»¶å¤¹çš„åç§°ã€‚
    # increment_pathå‡½æ•°çš„ä½œç”¨æ˜¯åœ¨è·¯å¾„åé¢æ·»åŠ ä¸€ä¸ªç´¢å¼•ä»¥ç¡®ä¿ç›®å½•çš„å”¯ä¸€æ€§ã€‚exist_okå‚æ•°è¡¨ç¤ºå¦‚æœç›®å½•å·²å­˜åœ¨ï¼Œæ˜¯å¦å…è®¸è¦†ç›–
    save_dir = increment_path(Path(project) / name, exist_ok=exist_ok)  # increment run
    # è¿™è¡Œä»£ç æ ¹æ®æ˜¯å¦ä¿å­˜æ–‡æœ¬æ–‡ä»¶ï¼ˆlabelsï¼‰ï¼Œåˆ›å»ºç›®å½•ã€‚å¦‚æœsave_txtä¸ºTrueï¼Œ
    # é‚£ä¹ˆåœ¨save_dirè·¯å¾„ä¸‹åˆ›å»ºä¸€ä¸ªåä¸º'labels'çš„å­ç›®å½•ï¼Œç”¨äºä¿å­˜æ–‡æœ¬æ–‡ä»¶ã€‚å¦‚æœsave_txtä¸ºFalseï¼Œå°±ç›´æ¥åœ¨save_dirè·¯å¾„ä¸‹åˆ›å»ºç›®å½•
    (save_dir / 'labels' if save_txt else save_dir).mkdir(parents=True, exist_ok=True)  # make dir

    # Load model
    device = select_device(device)
    # è¿™è¡Œä»£ç åˆ›å»ºä¸€ä¸ªç›®æ ‡æ£€æµ‹æ¨¡å‹ï¼ŒåŸºäºDetectMultiBackendç±»ã€‚å®ƒä½¿ç”¨äº†ä¸€äº›å‚æ•°ï¼Œå¦‚weightsï¼ˆæ¨¡å‹æƒé‡è·¯å¾„ï¼‰ã€deviceï¼ˆè®¡ç®—è®¾å¤‡ï¼‰ã€dnnã€dataå’Œfp16ï¼ˆæ˜¯å¦ä½¿ç”¨åŠç²¾åº¦æµ®ç‚¹æ•°è¿ç®—ï¼‰
    model = DetectMultiBackend(weights, device=device, dnn=dnn, data=data, fp16=half)
    # è¿™è¡Œä»£ç ä»modelå¯¹è±¡ä¸­è·å–äº†strideï¼ˆæ­¥é•¿ï¼‰ã€namesï¼ˆç±»åˆ«åç§°ï¼‰å’Œptï¼ˆæ¨¡å‹æƒé‡ï¼‰çš„å€¼ã€‚è¿™äº›å€¼å¯èƒ½åœ¨åç»­çš„æ“ä½œä¸­ä½¿ç”¨
    stride, names, pt = model.stride, model.names, model.pt
    # è¿™è¡Œä»£ç è°ƒç”¨check_img_sizeå‡½æ•°ï¼Œç”¨äºæ£€æŸ¥è¾“å…¥çš„å›¾åƒå°ºå¯¸ï¼ˆimgszï¼‰ã€‚å®ƒå¯èƒ½ä¼šè°ƒæ•´å›¾åƒå°ºå¯¸ï¼Œä»¥ç¡®ä¿å®ƒé€‚åˆæ¨¡å‹çš„è¦æ±‚ï¼ŒåŒæ—¶è€ƒè™‘åˆ°strideçš„å½±å“
    imgsz = check_img_size(imgsz, s=stride)  # check image size

    # Dataloader
    bs = 1  # batch_size
    if webcam:
        # ç”¨äºæ£€æŸ¥æ˜¯å¦æ˜¾ç¤ºå›¾åƒã€‚å‚æ•°warn=Trueè¡¨ç¤ºåœ¨æ²¡æœ‰GUIç•Œé¢æ—¶ä¼šæ˜¾ç¤ºè­¦å‘Šä¿¡æ¯
        view_img = check_imshow(warn=True)
        # å°†åˆ›å»ºä¸€ä¸ªLoadStreamsæ•°æ®é›†ï¼Œç”¨äºåŠ è½½æ‘„åƒå¤´æµã€‚img_sizeè¡¨ç¤ºå›¾åƒå°ºå¯¸ï¼Œstrideè¡¨ç¤ºæ­¥é•¿ï¼Œautoå’Œvid_strideå¯èƒ½ä¼šè¢«ç”¨æ¥è®¾ç½®ä¸åŒçš„å‚æ•°
        dataset = LoadStreams(source, img_size=imgsz, stride=stride, auto=pt, vid_stride=vid_stride)
        # è¿™é‡Œå°†æ‰¹é‡å¤§å°ï¼ˆbsï¼‰è®¾ç½®ä¸ºæ•°æ®é›†çš„é•¿åº¦ã€‚è¿™å¯èƒ½æ˜¯ä¸ºäº†åœ¨æ‘„åƒå¤´æ¨¡å¼ä¸‹ï¼Œæ¯ä¸ªæ‰¹æ¬¡éƒ½åŒ…å«ä¸€å¸§å›¾åƒ
        bs = len(dataset)
    elif screenshot:
        # å¦‚æœä½¿ç”¨å±å¹•æˆªå›¾ï¼Œå°†åˆ›å»ºä¸€ä¸ªLoadScreenshotsæ•°æ®é›†ï¼Œç”¨äºåŠ è½½å±å¹•æˆªå›¾
        dataset = LoadScreenshots(source, img_size=imgsz, stride=stride, auto=pt)
    else:
        # åŠ è½½å›¾åƒæ•°æ®
        dataset = LoadImages(source, img_size=imgsz, stride=stride, auto=pt, vid_stride=vid_stride)
    # åˆ›å»ºäº†ä¸¤ä¸ªåˆ—è¡¨ï¼Œvid_pathå’Œvid_writerï¼Œé•¿åº¦ä¸ºæ‰¹é‡å¤§å°ï¼ˆbsï¼‰ã€‚è¿™äº›åˆ—è¡¨å¯èƒ½ä¼šåœ¨åç»­ç”¨äºå­˜å‚¨è§†é¢‘è·¯å¾„å’Œå†™å…¥è§†é¢‘çš„å¯¹è±¡
    vid_path, vid_writer = [None] * bs, [None] * bs

    # Run inference
    # è¿™è¡Œä»£ç ç”¨äºé¢„çƒ­æ¨¡å‹ï¼Œä»¥ä¾¿æ›´å¥½åœ°è¿è¡Œæ¨ç†ã€‚å®ƒè®¾ç½®äº†é¢„çƒ­çš„å›¾åƒå°ºå¯¸
    model.warmup(imgsz=(1 if pt or model.triton else bs, 3, *imgsz))  # warmup
    # è¿™é‡Œåˆå§‹åŒ–äº†ä¸€äº›å˜é‡ï¼ŒåŒ…æ‹¬seenï¼ˆå·²å¤„ç†çš„å›¾åƒæ•°é‡ï¼‰ã€windowsï¼ˆçª—å£ä¿¡æ¯åˆ—è¡¨ï¼‰å’Œdtï¼ˆè®¡æ—¶å™¨çš„å…ƒç»„ï¼‰
    seen, windows, dt = 0, [], (Profile(), Profile(), Profile())
    # å°†è¾“å…¥å›¾åƒå‡†å¤‡å¥½ï¼Œç„¶åæ‰§è¡Œç›®æ ‡æ£€æµ‹æ¨ç†ã€‚åœ¨å¾ªç¯ä¸­ï¼Œå®ƒéå†æ•°æ®é›†ä¸­çš„æ¯ä¸ªå›¾åƒï¼Œå°†å…¶å‡†å¤‡å¥½ï¼Œå¹¶ä½¿ç”¨æ¨¡å‹è¿›è¡Œæ¨ç†ã€‚
    # è¿™æ˜¯ä¸€ä¸ªå¾ªç¯ï¼Œç”¨äºéå†æ•°æ®é›†ä¸­çš„æ¯ä¸ªå›¾åƒã€‚åœ¨æ¯æ¬¡è¿­ä»£ä¸­ï¼Œä¼šæå–pathï¼ˆå›¾åƒè·¯å¾„ï¼‰ã€imï¼ˆå›¾åƒæ•°ç»„ï¼‰ã€im0sï¼ˆåŸå§‹å›¾åƒæ•°ç»„ï¼Œæœªè°ƒæ•´å¤§å°ï¼‰ã€vid_capï¼ˆè§†é¢‘æ•è·å¯¹è±¡ï¼‰å’Œsï¼ˆå›¾åƒçš„æ­¥é•¿ï¼‰
    for path, im, im0s, vid_cap, s in dataset:
        # ä½¿ç”¨è®¡æ—¶å™¨æ¥æµ‹é‡æ¨ç†æ—¶é—´
        with dt[0]:
            # å°†imä»NumPyæ•°ç»„è½¬æ¢ä¸ºPyTorchå¼ é‡ï¼Œå¹¶å°†å…¶ç§»åŠ¨åˆ°æ¨¡å‹æ‰€åœ¨çš„è®¾å¤‡ï¼ˆCPUæˆ–GPUï¼‰
            im = torch.from_numpy(im).to(model.device)
            # å¦‚æœæ¨¡å‹ä½¿ç”¨åŠç²¾åº¦æµ®ç‚¹æ•°è®¡ç®—ï¼ˆfp16=Trueï¼‰ï¼Œåˆ™å°†å›¾åƒå¼ é‡è½¬æ¢ä¸ºåŠç²¾åº¦æµ®ç‚¹æ•°ï¼ˆfloat16ï¼‰ã€‚å¦åˆ™ï¼Œå°†å…¶è½¬æ¢ä¸ºå•ç²¾åº¦æµ®ç‚¹æ•°ï¼ˆfloat32ï¼‰
            im = im.half() if model.fp16 else im.float()  # uint8 to fp16/32
            # å°†å›¾åƒçš„åƒç´ å€¼ä»0-255çš„èŒƒå›´æ˜ å°„åˆ°0.0-1.0çš„èŒƒå›´
            im /= 255  # 0 - 255 to 0.0 - 1.0
            # è¿™è¡Œä»£ç æ£€æŸ¥å›¾åƒå¼ é‡æ˜¯å¦ç¼ºå°‘æ‰¹é‡ç»´åº¦ï¼ˆbatch dimensionï¼‰ï¼Œå¦‚æœæ˜¯ï¼Œåˆ™åœ¨ç»´åº¦0å¤„æ·»åŠ ä¸€ä¸ªæ‰¹é‡ç»´åº¦
            if len(im.shape) == 3:
                im = im[None]  # expand for batch dim

        # Inference
        # æ‰§è¡Œç›®æ ‡æ£€æµ‹æ¨ç†å¹¶è¿›è¡Œåå¤„ç†ï¼ŒåŒ…æ‹¬å¯è§†åŒ–å’Œéæœ€å¤§æŠ‘åˆ¶ã€‚åœ¨æ¯æ¬¡è¿­ä»£ä¸­ï¼Œå®ƒå°†å›¾åƒä¼ é€’ç»™æ¨¡å‹è¿›è¡Œæ¨ç†ï¼Œç„¶åå¯¹é¢„æµ‹ç»“æœè¿›è¡Œå¤„ç†ä»¥å¾—åˆ°æœ€ç»ˆçš„æ£€æµ‹ç»“æœ
        # ä½¿ç”¨è®¡æ—¶å™¨æ¥æµ‹é‡æ¨ç†æ—¶é—´
        with dt[1]:
            # æ ¹æ®visualizeå˜é‡çš„å€¼æ¥è®¾ç½®æ˜¯å¦å¯è§†åŒ–ã€‚å¦‚æœvisualizeä¸ºTrueï¼Œå°†ä½¿ç”¨increment_pathå‡½æ•°åˆ›å»ºä¸€ä¸ªä¿å­˜å¯è§†åŒ–ç»“æœçš„ç›®å½•
            visualize = increment_path(save_dir / Path(path).stem, mkdir=True) if visualize else False
            # å°†å›¾åƒä¼ é€’ç»™æ¨¡å‹è¿›è¡Œæ¨ç†ã€‚å®ƒä¼šè¿”å›é¢„æµ‹ç»“æœï¼Œå…¶ä¸­åŒ…æ‹¬é¢„æµ‹æ¡†ã€ç½®ä¿¡åº¦ã€ç±»åˆ«ç­‰ä¿¡æ¯ã€‚augmentå‚æ•°è¡¨ç¤ºæ˜¯å¦ä½¿ç”¨æ•°æ®å¢å¼ºï¼Œvisualizeå‚æ•°è¡¨ç¤ºæ˜¯å¦è¿›è¡Œå¯è§†åŒ–
            pred = model(im, augment=augment, visualize=visualize)

        # NMS
        # ä½¿ç”¨è®¡æ—¶å™¨æ¥æµ‹é‡éæœ€å¤§æŠ‘åˆ¶ï¼ˆNMSï¼‰çš„æ—¶é—´
        with dt[2]:
            # è¿™è¡Œä»£ç ä½¿ç”¨éæœ€å¤§æŠ‘åˆ¶ç®—æ³•å¯¹é¢„æµ‹ç»“æœè¿›è¡Œåå¤„ç†ï¼Œä»¥å»é™¤é‡å çš„æ¡†å¹¶ä¿ç•™æœ€ç›¸å…³çš„æ¡†ã€‚å®ƒä½¿ç”¨äº†ä¸€äº›å‚æ•°ï¼Œ
            # å¦‚ç½®ä¿¡åº¦é˜ˆå€¼ï¼ˆconf_thresï¼‰ã€IoUé˜ˆå€¼ï¼ˆiou_thresï¼‰ã€ç±»åˆ«ä¿¡æ¯ï¼ˆclassesï¼‰ã€æ˜¯å¦ä½¿ç”¨æ— å…³ç±»åˆ«NMSï¼ˆagnostic_nmsï¼‰ä»¥åŠæœ€å¤§æ£€æµ‹æ•°ï¼ˆmax_detï¼‰ç­‰
            pred = non_max_suppression(pred, conf_thres, iou_thres, classes, agnostic_nms, max_det=max_det)

        # Second-stage classifier (optional)
        # è´Ÿè´£æ‰§è¡Œç¬¬äºŒé˜¶æ®µçš„åˆ†ç±»å™¨ï¼ˆå¦‚æœä½¿ç”¨äº†çš„è¯ï¼‰ä»¥åŠå¤„ç†æ£€æµ‹é¢„æµ‹ç»“æœ
        # æš—ç¤ºåœ¨æ­¤å¯æ‰§è¡Œç¬¬äºŒé˜¶æ®µçš„åˆ†ç±»å™¨ã€‚æ ¹æ®æ³¨é‡Šï¼Œè¿™æ˜¯ä¸€ä¸ªå¯é€‰æ­¥éª¤ï¼Œå¯èƒ½æ¶‰åŠå°†åˆ†ç±»å™¨åº”ç”¨äºé¢„æµ‹ç»“æœ
        # pred = utils.general.apply_classifier(pred, classifier_model, im, im0s)

        # Process predictions
        # ä½œç”¨æ˜¯å¯¹æ£€æµ‹ç»“æœè¿›è¡Œå¤„ç†ï¼ŒåŒ…æ‹¬åœ¨å›¾åƒä¸Šç»˜åˆ¶æ¡†å’Œæ ‡ç­¾ï¼Œä¿å­˜æ ‡ç­¾æ–‡ä»¶ä»¥åŠä¿å­˜è£å‰ªå›¾åƒæ–‡ä»¶ã€‚æ ¹æ®éœ€æ±‚ï¼Œå®ƒå¯ä»¥åœ¨å›¾åƒä¸Šç»˜åˆ¶æ£€æµ‹ç»“æœï¼Œä¿å­˜å›¾åƒå’Œæ ‡ç­¾ï¼Œä»¥åŠè¿›è¡Œä¸€äº›åå¤„ç†æ“ä½œ
        # è¿™ä¸ªå¾ªç¯éå†æ¯å¼ å›¾åƒçš„é¢„æµ‹ç»“æœã€‚åœ¨æ¯æ¬¡è¿­ä»£ä¸­ï¼Œiè¡¨ç¤ºç´¢å¼•ï¼Œdetè¡¨ç¤ºä¸€å¼ å›¾åƒçš„é¢„æµ‹ç»“æœï¼ˆå¯èƒ½åŒ…å«å¤šä¸ªæ£€æµ‹æ¡†ï¼‰
        for i, det in enumerate(pred):  # per image
            # å¢åŠ å·²å¤„ç†çš„å›¾åƒæ•°é‡
            seen += 1
            if webcam:  # batch_size >= 1
                # å¦‚æœæ˜¯æ‘„åƒå¤´æ¨¡å¼ï¼ˆwebcamä¸ºTrueï¼‰ï¼Œåˆ™å¯¹æ‰¹é‡ä¸­çš„æ¯å¼ å›¾åƒè¿›è¡Œå¤„ç†ï¼Œæå–è·¯å¾„ã€åŸå§‹å›¾åƒå’Œå¸§æ•°ç­‰ä¿¡æ¯ï¼Œå¹¶å°†è¿™äº›ä¿¡æ¯è®°å½•åœ¨så˜é‡ä¸­
                p, im0, frame = path[i], im0s[i].copy(), dataset.count
                s += f'{i}: '
            else:
                # å¦‚æœä¸æ˜¯æ‘„åƒå¤´æ¨¡å¼ï¼Œé‚£ä¹ˆå°†ç›´æ¥æå–è·¯å¾„ã€åŸå§‹å›¾åƒå’Œå¸§æ•°ç­‰ä¿¡æ¯
                p, im0, frame = path, im0s.copy(), getattr(dataset, 'frame', 0)

            # å°†è·¯å¾„pè½¬æ¢ä¸ºPathå¯¹è±¡
            p = Path(p)  # to Path
            # æ„å»ºä¿å­˜å›¾åƒæ–‡ä»¶çš„è·¯å¾„ï¼Œå…¶ä¸­save_diræ˜¯ä¿å­˜ç›®å½•ï¼Œp.nameæ˜¯æ–‡ä»¶å
            save_path = str(save_dir / p.name)  # im.jpg
            # æ„å»ºä¿å­˜æ ‡ç­¾æ–‡ä»¶çš„è·¯å¾„ï¼Œå…¶ä¸­save_diræ˜¯ä¿å­˜ç›®å½•ï¼Œp.stemæ˜¯æ–‡ä»¶åçš„ä¸»å¹²éƒ¨åˆ†ï¼Œdataset.modeæ˜¯æ•°æ®é›†æ¨¡å¼ï¼ˆå¯èƒ½æ˜¯'image'æˆ–'video'ï¼‰ï¼Œframeæ˜¯å¸§æ•°
            txt_path = str(save_dir / 'labels' / p.stem) + ('' if dataset.mode == 'image' else f'_{frame}')  # im.txt
            # å°†å›¾åƒçš„å®½é«˜ä¿¡æ¯æ·»åŠ åˆ°å­—ç¬¦ä¸²sä¸­
            s += '%gx%g ' % im.shape[2:]  # print string
            # æ„å»ºä¸€ä¸ªå¼ é‡gnï¼Œè¡¨ç¤ºå½’ä¸€åŒ–å¢ç›Šï¼Œç”¨äºä»å›¾åƒå°ºå¯¸è½¬æ¢åˆ°å½’ä¸€åŒ–å°ºå¯¸
            gn = torch.tensor(im0.shape)[[1, 0, 1, 0]]  # normalization gain whwh
            # å¦‚æœsave_cropä¸ºTrueï¼Œå°†åŸå§‹å›¾åƒå¤åˆ¶ç»™imcï¼Œå¦åˆ™ç›´æ¥èµ‹å€¼
            imc = im0.copy() if save_crop else im0  # for save_crop
            # åˆ›å»ºä¸€ä¸ªæ³¨é‡Šå™¨å¯¹è±¡annotatorï¼Œç”¨äºåœ¨å›¾åƒä¸Šç»˜åˆ¶è¾¹ç•Œæ¡†å’Œæ ‡ç­¾ã€‚line_widthè¡¨ç¤ºçº¿çš„å®½åº¦ï¼Œexampleæ˜¯ç±»åˆ«åç§°çš„ç¤ºä¾‹
            annotator = Annotator(im0, line_width=line_thickness, example=str(names))
            # å¦‚æœæ£€æµ‹ç»“æœä¸ä¸ºç©º
            if len(det):
                # Rescale boxes from img_size to im0 size
                # å°†æ£€æµ‹æ¡†ä»imçš„å°ºå¯¸ç¼©æ”¾åˆ°åŸå§‹å›¾åƒim0çš„å°ºå¯¸
                det[:, :4] = scale_boxes(im.shape[2:], det[:, :4], im0.shape).round()

                # Print results
                # éå†ä¸åŒçš„ç±»åˆ«ï¼Œç»Ÿè®¡æ¯ä¸ªç±»åˆ«çš„æ£€æµ‹æ•°é‡ï¼Œå¹¶å°†ç»“æœæ·»åŠ åˆ°å­—ç¬¦ä¸²sä¸­
                for c in det[:, 5].unique():
                    n = (det[:, 5] == c).sum()  # detections per class
                    s += f"{n} {names[int(c)]}{'s' * (n > 1)}, "  # add to string

                # Write results
                # éå†æ¯ä¸ªæ£€æµ‹ç»“æœï¼Œå¯¹æ¯ä¸ªæ¡†è¿›è¡Œå¤„ç†
                for *xyxy, conf, cls in reversed(det):
                    if save_txt:  # Write to file
                        xywh = (xyxy2xywh(torch.tensor(xyxy).view(1, 4)) / gn).view(-1).tolist()  # normalized xywh
                        line = (cls, *xywh, conf) if save_conf else (cls, *xywh)  # label format
                        with open(f'{txt_path}.txt', 'a') as f:
                            f.write(('%g ' * len(line)).rstrip() % line + '\n')

                    if save_img or save_crop or view_img:  # Add bbox to image
                        c = int(cls)  # integer class
                        label = None if hide_labels else (names[c] if hide_conf else f'{names[c]} {conf:.2f}')
                        annotator.box_label(xyxy, label, color=colors(c, True))
                    if save_crop:
                        save_one_box(xyxy, imc, file=save_dir / 'crops' / names[c] / f'{p.stem}.jpg', BGR=True)

            # Stream results
            # åœ¨å›¾åƒä¸Šå±•ç¤ºæ£€æµ‹ç»“æœï¼Œä»¥å®æ—¶æµå¼æ–¹å¼æ˜¾ç¤ºã€‚å¦‚æœæ‚¨éœ€è¦å®æ—¶æŸ¥çœ‹ç›®æ ‡æ£€æµ‹ç»“æœï¼Œè¿™éƒ¨åˆ†ä»£ç ä¼šéå¸¸æœ‰ç”¨
            # è·å–ç»è¿‡æ³¨é‡Šå™¨å¤„ç†åçš„å›¾åƒï¼Œå³å¸¦æœ‰ç»˜åˆ¶çš„è¾¹ç•Œæ¡†å’Œæ ‡ç­¾çš„å›¾åƒ
            im0 = annotator.result()
            # å¦‚æœå¯ç”¨äº†å›¾åƒæŸ¥çœ‹
            if view_img:
                # ä»¶åˆ¤æ–­ç”¨äºæ£€æŸ¥å½“å‰æ“ä½œç³»ç»Ÿæ˜¯å¦ä¸ºLinuxï¼Œå¹¶ä¸”çª—å£åç§°ï¼ˆpï¼‰æ˜¯å¦å·²ç»å­˜åœ¨äºwindowsåˆ—è¡¨ä¸­
                if platform.system() == 'Linux' and p not in windows:
                    windows.append(p)
                    # å¦‚æœæ˜¯Linuxç³»ç»Ÿï¼Œå¹¶ä¸”çª—å£åç§°ä¸åœ¨windowsåˆ—è¡¨ä¸­ï¼Œé‚£ä¹ˆåˆ›å»ºä¸€ä¸ªå¸¦æœ‰çª—å£åç§°çš„OpenCVçª—å£ã€‚è¿™å…è®¸åœ¨Linuxä¸Šè°ƒæ•´çª—å£å¤§å°
                    cv2.namedWindow(str(p), cv2.WINDOW_NORMAL | cv2.WINDOW_KEEPRATIO)  # allow window resize (Linux)
                    # è°ƒæ•´OpenCVçª—å£çš„å¤§å°ï¼Œä»¥é€‚åº”å›¾åƒçš„å°ºå¯¸
                    cv2.resizeWindow(str(p), im0.shape[1], im0.shape[0])
                # åœ¨OpenCVçª—å£ä¸­æ˜¾ç¤ºå›¾åƒ
                cv2.imshow(str(p), im0)
                cv2.waitKey(1)  # 1 millisecond

            # Save results (image with detections)
            # ä¿å­˜å¸¦æœ‰æ£€æµ‹ç»“æœçš„å›¾åƒï¼Œå¹¶æ‰“å°å‡ºæ¨ç†æ—¶é—´å’Œä¸€äº›ä¿¡æ¯ã€‚å¦‚æœéœ€è¦å°†æ£€æµ‹ç»“æœä¿å­˜ä¸ºå›¾åƒæˆ–è§†é¢‘æ–‡ä»¶ï¼Œä»¥åŠæ‰“å°å‡ºç›¸å…³ä¿¡æ¯
            # å¦‚æœå¯ç”¨äº†å›¾åƒä¿å­˜
            if save_img:
                # æœæ•°æ®é›†æ¨¡å¼ä¸º'image'ï¼Œè¡¨ç¤ºå•å¼ å›¾åƒæ¨¡å¼
                if dataset.mode == 'image':
                    # ä½¿ç”¨OpenCVå°†å›¾åƒim0ä¿å­˜ä¸ºæ–‡ä»¶ï¼Œæ–‡ä»¶è·¯å¾„ä¸ºsave_path
                    cv2.imwrite(save_path, im0)
                # å¦åˆ™ï¼Œæ•°æ®é›†æ¨¡å¼å¯èƒ½æ˜¯'video'æˆ–'stream'
                else:  # 'video' or 'stream'
                    # å¦‚æœå½“å‰è§†é¢‘è·¯å¾„vid_path[i]ä¸ä¿å­˜è·¯å¾„save_pathä¸åŒï¼Œè¡¨ç¤ºå¼€å§‹å¤„ç†æ–°çš„è§†é¢‘
                    if vid_path[i] != save_path:  # new video
                        vid_path[i] = save_path
                        # å¦‚æœä¹‹å‰å·²ç»å­˜åœ¨è§†é¢‘å†™å…¥å¯¹è±¡vid_writer[i]ï¼Œåˆ™å…ˆé‡Šæ”¾å®ƒ
                        if isinstance(vid_writer[i], cv2.VideoWriter):
                            vid_writer[i].release()  # release previous video writer
                        # å¦‚æœæ˜¯è§†é¢‘ï¼ˆvid_capå­˜åœ¨ï¼‰ï¼Œè·å–è§†é¢‘çš„å¸§ç‡ã€å®½åº¦å’Œé«˜åº¦
                        if vid_cap:  # video
                            fps = vid_cap.get(cv2.CAP_PROP_FPS)
                            w = int(vid_cap.get(cv2.CAP_PROP_FRAME_WIDTH))
                            h = int(vid_cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
                        # å¦‚æœæ˜¯æµï¼ˆvid_capä¸å­˜åœ¨ï¼‰ï¼Œè®¾ç½®é»˜è®¤å¸§ç‡å’Œå°ºå¯¸
                        else:  # stream
                            fps, w, h = 30, im0.shape[1], im0.shape[0]
                        # ä¿å­˜è·¯å¾„æ›´æ”¹ä¸ºä»¥'.mp4'ä¸ºåç¼€çš„æ–°è·¯å¾„ï¼Œå¼ºåˆ¶ä½¿ç”¨MP4æ ¼å¼ä¿å­˜ç»“æœè§†é¢‘
                        save_path = str(Path(save_path).with_suffix('.mp4'))  # force *.mp4 suffix on results videos
                        # åˆ›å»ºä¸€ä¸ªæ–°çš„è§†é¢‘å†™å…¥å¯¹è±¡vid_writer[i]ï¼Œä½¿ç”¨MP4æ ¼å¼ã€æŒ‡å®šå¸§ç‡å’Œå°ºå¯¸
                        vid_writer[i] = cv2.VideoWriter(save_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (w, h))
                    # å°†å›¾åƒim0å†™å…¥è§†é¢‘å†™å…¥å¯¹è±¡vid_writer[i]ä¸­ï¼Œä»¥ç”Ÿæˆè§†é¢‘
                    vid_writer[i].write(im0)

        # Print time (inference-only)
        # æ‰“å°ä¿¡æ¯ï¼ŒåŒ…æ‹¬æ£€æµ‹çš„ç±»åˆ«å’Œæ•°é‡ï¼Œæ¨ç†æ—¶é—´ç­‰
        LOGGER.info(f"{s}{'' if len(det) else '(no detections), '}{dt[1].dt * 1E3:.1f}ms")

    # Print results
    # è¿™è¡Œä»£ç è®¡ç®—å‡ºæ¯ä¸ªé˜¶æ®µçš„å¹³å‡é€Ÿåº¦ï¼Œå•ä½ä¸ºæ¯«ç§’ï¼ˆmsï¼‰ï¼Œæ ¹æ®æ¯ä¸ªé˜¶æ®µçš„è®¡æ—¶å™¨å’Œå·²å¤„ç†å›¾åƒæ•°é‡
    t = tuple(x.t / seen * 1E3 for x in dt)  # speeds per image
    # æ‰“å°å‡ºæ¨ç†é€Ÿåº¦çš„ä¿¡æ¯ï¼ŒåŒ…æ‹¬é¢„å¤„ç†æ—¶é—´ã€æ¨ç†æ—¶é—´å’ŒNMSæ—¶é—´
    LOGGER.info(f'Speed: %.1fms pre-process, %.1fms inference, %.1fms NMS per image at shape {(1, 3, *imgsz)}' % t)
    if save_txt or save_img:
        # å¦‚æœå¯ç”¨äº†ä¿å­˜æ–‡æœ¬ï¼Œè¿™è¡Œä»£ç ç»Ÿè®¡å·²ä¿å­˜çš„æ–‡æœ¬æ–‡ä»¶æ•°é‡ï¼Œå¹¶ç”Ÿæˆç›¸åº”çš„æç¤ºä¿¡æ¯
        s = f"\n{len(list(save_dir.glob('labels/*.txt')))} labels saved to {save_dir / 'labels'}" if save_txt else ''
        LOGGER.info(f"Results saved to {colorstr('bold', save_dir)}{s}")

    '''
    åœ¨YOLOv5çš„detectå‡½æ•°ä¸­ï¼Œupdateå‚æ•°çš„ä½œç”¨å¦‚ä¸‹ï¼š
        å½“update=Trueæ—¶ï¼šä¼šåœ¨æ‰§è¡Œæ¨ç†çš„è¿‡ç¨‹ä¸­ï¼Œæ ¹æ®éœ€è¦å¯¹æ¨¡å‹è¿›è¡Œæ›´æ–°ï¼Œä»¥è§£å†³å¯èƒ½çš„æºä»£ç å˜æ›´è­¦å‘Šï¼ˆSourceChangeWarningï¼‰é—®é¢˜ã€‚
            æ¨¡å‹æ›´æ–°å¯èƒ½æ¶‰åŠå»é™¤ä¸å¿…è¦çš„ä¼˜åŒ–å™¨çŠ¶æ€ï¼Œä»¥ç¡®ä¿æ¨¡å‹åœ¨æ¨ç†è¿‡ç¨‹ä¸­æ­£å¸¸å·¥ä½œï¼Œè€Œä¸å—ä¼˜åŒ–å™¨çŠ¶æ€çš„å½±å“ã€‚è¿™å¯ä»¥å¸®åŠ©é¿å…åœ¨æ¨ç†æ—¶å‡ºç°ä¸å¿…è¦çš„é”™è¯¯ã€‚
        å½“update=Falseæ—¶ï¼šæ¨ç†è¿‡ç¨‹ä¸­ä¸ä¼šå¯¹æ¨¡å‹è¿›è¡Œæ›´æ–°ï¼Œä¼˜åŒ–å™¨çŠ¶æ€ä¿æŒä¸å˜ã€‚è¿™å¯èƒ½ä¼šåœ¨ä¸€äº›æƒ…å†µä¸‹å¯¼è‡´æ¨¡å‹åœ¨æ¨ç†è¿‡ç¨‹ä¸­å‡ºç°é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯å½“æºä»£ç å‘ç”Ÿå˜æ›´æ—¶ï¼Œå¯èƒ½ä¼šå‡ºç°ä¸€äº›è­¦å‘Šæˆ–é”™è¯¯ã€‚
    
    << SourceChangeWarning >>
    åœ¨Pythonä¸­ï¼ŒSourceChangeWarningæ˜¯ä¸€ç§è­¦å‘Šç±»å‹ï¼Œç”¨äºæŒ‡ç¤ºæºä»£ç çš„æ”¹å˜å¯èƒ½ä¼šå¯¼è‡´æŸäº›æ“ä½œæˆ–è¡Œä¸ºä¸å†é€‚ç”¨æˆ–ä¸å†å…·æœ‰é¢„æœŸçš„æ•ˆæœã€‚
        è¿™ç§è­¦å‘Šé€šå¸¸æ˜¯ç”±äºPythonè§£é‡Šå™¨çš„æ”¹å˜ã€åº“çš„æ›´æ–°æˆ–æºä»£ç çš„ä¿®æ”¹å¼•èµ·çš„ã€‚
    
    å½“Pythonè§£é‡Šå™¨æˆ–åº“çš„å¼€å‘äººå‘˜åœ¨æ–°ç‰ˆæœ¬ä¸­è¿›è¡Œäº†ä¸€äº›æ›´æ”¹ï¼Œå¹¶ä¸”è¿™äº›æ›´æ”¹å¯èƒ½ä¼šå½±å“åˆ°æ—§ç‰ˆæœ¬ä»£ç çš„æ­£ç¡®æ€§æˆ–è¡Œä¸ºæ—¶ï¼Œå°±ä¼šè§¦å‘SourceChangeWarningè­¦å‘Šã€‚  
        è¿™ç§è­¦å‘Šçš„ç›®çš„æ˜¯æé†’å¼€å‘äººå‘˜åœ¨æ›´æ–°åº“æˆ–æºä»£ç æ—¶è¦æ³¨æ„æ½œåœ¨çš„é—®é¢˜ï¼Œä»¥ä¾¿åŠæ—¶ä¿®å¤æˆ–è°ƒæ•´ä»£ç ä»¥é€‚åº”å˜æ›´
        
    å¯¹äºYOLOv5ä¸­çš„detectå‡½æ•°ï¼Œupdateå‚æ•°çš„ä½œç”¨æ˜¯ä¸ºäº†é¿å…åœ¨æ¨ç†è¿‡ç¨‹ä¸­ç”±äºæºä»£ç å˜æ›´è€Œå¼•å‘çš„è­¦å‘Šæˆ–é”™è¯¯ã€‚
        é€šè¿‡åœ¨æ¨ç†è¿‡ç¨‹ä¸­æ›´æ–°æ¨¡å‹ï¼Œå¯ä»¥ç¡®ä¿æ¨¡å‹çš„çŠ¶æ€ä¸æºä»£ç å˜æ›´ä¿æŒä¸€è‡´ï¼Œä»è€Œå‡å°‘å¯èƒ½çš„é—®é¢˜ã€‚è¿™æ ·åšå¯ä»¥å¸®åŠ©å¼€å‘äººå‘˜åœ¨è¿›è¡Œç›®æ ‡æ£€æµ‹æ¨ç†æ—¶è·å¾—æ›´ç¨³å®šçš„ç»“æœã€‚
    '''
    # å¦‚æœå¯ç”¨äº†æ›´æ–°æ“ä½œ
    if update:
        # è°ƒç”¨strip_optimizerå‡½æ•°ï¼Œç”¨äºæ›´æ–°æ¨¡å‹ä»¥è§£å†³æ½œåœ¨çš„æºä»£ç å˜æ›´è­¦å‘Šï¼ˆSourceChangeWarningï¼‰é—®é¢˜
        strip_optimizer(weights[0])  # update model (to fix SourceChangeWarning)


def parse_opt():
    parser = argparse.ArgumentParser()
    # parser.add_argument('--weights', nargs='+', type=str, default=ROOT / 'yolov5s.pt', help='model path or triton URL')
    # parser.add_argument('--source', type=str, default=ROOT / 'data/images', help='file/dir/URL/glob/screen/0(webcam)')
    # parser.add_argument('--data', type=str, default=ROOT / 'data/coco128.yaml', help='(optional) datasets.yaml path')
    # parser.add_argument('--imgsz', '--img', '--img-size', nargs='+', type=int, default=[640], help='inference size h,w')
    # parser.add_argument('--conf-thres', type=float, default=0.25, help='confidence threshold')
    # parser.add_argument('--iou-thres', type=float, default=0.45, help='NMS IoU threshold')
    # parser.add_argument('--max-det', type=int, default=1000, help='maximum detections per image')
    # parser.add_argument('--device', default='', help='cuda device, i.e. 0 or 0,1,2,3 or cpu')
    # parser.add_argument('--view-img', action='store_true', help='show results')
    # parser.add_argument('--save-txt', action='store_true', help='save results to *.txt')
    # parser.add_argument('--save-conf', action='store_true', help='save confidences in --save-txt labels')
    # parser.add_argument('--save-crop', action='store_true', help='save cropped prediction boxes')
    # parser.add_argument('--nosave', action='store_true', help='do not save images/videos')
    # parser.add_argument('--classes', nargs='+', type=int, help='filter by class: --classes 0, or --classes 0 2 3')
    # parser.add_argument('--agnostic-nms', action='store_true', help='class-agnostic NMS')
    # parser.add_argument('--augment', action='store_true', help='augmented inference')
    # parser.add_argument('--visualize', action='store_true', help='visualize features')
    # parser.add_argument('--update', action='store_true', help='update all models')
    # parser.add_argument('--project', default=ROOT / 'runs/detect', help='save results to project/name')
    # parser.add_argument('--name', default='exp', help='save results to project/name')
    # parser.add_argument('--exist-ok', action='store_true', help='existing project/name ok, do not increment')
    # parser.add_argument('--line-thickness', default=3, type=int, help='bounding box thickness (pixels)')
    # parser.add_argument('--hide-labels', default=False, action='store_true', help='hide labels')
    # parser.add_argument('--hide-conf', default=False, action='store_true', help='hide confidences')
    # parser.add_argument('--half', action='store_true', help='use FP16 half-precision inference')
    # parser.add_argument('--dnn', action='store_true', help='use OpenCV DNN for ONNX inference')
    # parser.add_argument('--vid-stride', type=int, default=1, help='video frame-rate stride')
    parser.add_argument('--weights', nargs='+', type=str, default=ROOT / 'runs/train/5mNAMC3/weights/best_openvino_model_int8', help='model path or triton URL')
    parser.add_argument('--source', type=str, default=r'D:\StuData\tomato\dataset_factory\background\coco_2\images', help='file/dir/URL/glob/screen/0(webcam)')
    parser.add_argument('--data', type=str, default=ROOT / 'data/coco128.yaml', help='(optional) datasets.yaml path')
    parser.add_argument('--imgsz', '--img', '--img-size', nargs='+', type=int, default=[640], help='inference size h,w')
    parser.add_argument('--conf-thres', type=float, default=0.25, help='confidence threshold')
    parser.add_argument('--iou-thres', type=float, default=0.45, help='NMS IoU threshold')
    parser.add_argument('--max-det', type=int, default=1000, help='maximum detections per image')
    parser.add_argument('--device', default='', help='cuda device, i.e. 0 or 0,1,2,3 or cpu')
    parser.add_argument('--view-img', action='store_true', help='show results')
    parser.add_argument('--save-txt', action='store_true', help='save results to *.txt')
    parser.add_argument('--save-conf', action='store_true', help='save confidences in --save-txt labels')
    parser.add_argument('--save-crop', action='store_true', help='save cropped prediction boxes')
    parser.add_argument('--nosave', action='store_true', help='do not save images/videos')
    parser.add_argument('--classes', nargs='+', type=int, help='filter by class: --classes 0, or --classes 0 2 3')
    parser.add_argument('--agnostic-nms', action='store_true', help='class-agnostic NMS')
    parser.add_argument('--augment', action='store_true', help='augmented inference')
    parser.add_argument('--visualize', action='store_true', help='visualize features')
    parser.add_argument('--update', action='store_true', help='update all models')
    parser.add_argument('--project', default=ROOT / 'runs/detect', help='save results to project/name')
    parser.add_argument('--name', default='exp', help='save results to project/name')
    parser.add_argument('--exist-ok', action='store_true', help='existing project/name ok, do not increment')
    parser.add_argument('--line-thickness', default=3, type=int, help='bounding box thickness (pixels)')
    parser.add_argument('--hide-labels', default=False, action='store_true', help='hide labels')
    parser.add_argument('--hide-conf', default=False, action='store_true', help='hide confidences')
    parser.add_argument('--half', action='store_true', help='use FP16 half-precision inference')
    parser.add_argument('--dnn', action='store_true', help='use OpenCV DNN for ONNX inference')
    parser.add_argument('--vid-stride', type=int, default=1, help='video frame-rate stride')
    opt = parser.parse_args()
    opt.imgsz *= 2 if len(opt.imgsz) == 1 else 1  # expand
    print_args(vars(opt))
    return opt


def main(opt):
    check_requirements(ROOT / 'requirements.txt', exclude=('tensorboard', 'thop'))
    run(**vars(opt))


if __name__ == '__main__':
    opt = parse_opt()
    main(opt)
